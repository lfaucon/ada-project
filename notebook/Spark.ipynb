{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA - Project\n",
    "## With Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"ADA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4950"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.parallelize(range(100))\n",
    "data.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you received 4950 as a result, your spark is working well :) Good job !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"session\":\"progfun-002\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and parsing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Event Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PROBLEM_EVENTS ---\n",
      "0 EventID / 1 ForumUserID / 2 MaximumSubmissions / 3 AccountUserID / 4 SubmissionNumber / 5 Grade / 6 TimeStamp / 7 DataPackageID / 8 ProblemID / 9 SoftCloseTime / 10 ProblemType / 11 HardCloseTime / 12 Platform / 13 OpenTime / 14 EventType / 15 Title / 16 SessionUserID / 17 UniqueProblemID / 18 UniqueUserID / \n",
      "\n",
      "--- VIDEO_EVENTS ---\n",
      "0 EventID / 1 ForumUserID / 2 OldTime / 3 AccountUserID / 4 CurrentTime / 5 SeekType / 6 TimeStamp / 7 DataPackageID / 8 UniqueRowID / 9 TableName / 10 VideoID / 11 Platform / 12 NewSpeed / 13 EventSource / 14 EventType / 15 SessionUserID / 16 NewTime / 17 OldSpeed / \n",
      "\n",
      "--- FORUM_EVENTS ---\n",
      "0 EventID / 1 ForumUserID / 2 PostID / 3 AccountUserID / 4 TimeStamp / 5 DataPackageID / 6 UniqueRowID / 7 TableName / 8 Platform / 9 EventSource / 10 PostType / 11 EventType / 12 JoinID / 13 SessionUserID / "
     ]
    }
   ],
   "source": [
    "# Reading csv files: Create RDD () with one string entry per line in the file\n",
    "rdd_problem_events = sc.textFile(\"../data/\"+config['session']+\"_Problem_Events_with_Info.csv\")\n",
    "rdd_video_events = sc.textFile(\"../data/\"+config['session']+\"_Video_Events.csv\")\n",
    "rdd_forum_events = sc.textFile(\"../data/\"+config['session']+\"_Forum_Events.csv\")\n",
    "\n",
    "# Prints the first line (header) along with indices for each table\n",
    "print(\"--- PROBLEM_EVENTS ---\")\n",
    "for idx,field in enumerate(rdd_problem_events.first().split(\",\")): \n",
    "    print(idx,field, end=\" / \")\n",
    "\n",
    "print(\"\\n\\n--- VIDEO_EVENTS ---\")\n",
    "for idx,field in enumerate(rdd_video_events.first().split(\",\")): \n",
    "    print(idx,field, end=\" / \")\n",
    "\n",
    "print(\"\\n\\n--- FORUM_EVENTS ---\")\n",
    "for idx,field in enumerate(rdd_forum_events.first().split(\",\")): \n",
    "    print(idx,field, end=\" / \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3bcd1a54ed6ddb04b4a6fb2906110a01,None,0,None,7,None,1365344171,progfun-002,7,2147483640,Video,2147483640,Coursera,32400,Problem.Check,Lecture 1.2 - Elements of Programming (14:25),c4d4e5fcd2feba9f3234ee8d852dc7b22fbc07e4,f322944718b2ee0e53292118111533c7,21f13b3f6b50a83343b57d2f1d07dbdf \n",
      "\n",
      "db75adce6b87e7ab79242ea0af4b82d4,None,154.696,None,154.697,None,1372391638,progfun-002,00000078c0f0685cc50a25a8d5734a88,Video_Events,33,coursera,1.0,None,Video.Play,ef64fb7b096008f7eaf8441684afdf99af9af54a,None,1.0 \n",
      "\n",
      "f3fdb52859b2511308aee554a573194e,None,17,4108315,1376254235,progfun-002,000006c12322ca29c7013dac42ef1a6a,Forum_Events,coursera,None,Thread,Forum.Thread.View,03b1fa287de5ef57d9c8482195b5167f,None \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the filter method to remove the first line\n",
    "rdd_problem_events = rdd_problem_events.filter(lambda x: not x.startswith('EventID'))\n",
    "rdd_video_events = rdd_video_events.filter(lambda x: not x.startswith('EventID'))\n",
    "rdd_forum_events = rdd_forum_events.filter(lambda x: not x.startswith('EventID'))\n",
    "\n",
    "# Prints first record for each table\n",
    "print(rdd_problem_events.first(),\"\\n\") \n",
    "print(rdd_video_events.first(),\"\\n\") \n",
    "print(rdd_forum_events.first(),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to parse the string entries from previous dataset\n",
    "def parse_problems(x):\n",
    "    data = x.split(',')\n",
    "    return {\n",
    "        'Grade':float(data[5]),\n",
    "        'TimeStamp':int(data[6]),\n",
    "        'Date':pd.to_datetime(int(data[6]),unit='s'),\n",
    "        'ProblemID':int(data[8]),\n",
    "        'ProblemType':data[10],\n",
    "        'EventType':data[14].split('.')[0],\n",
    "        'EventSubType':data[14].split('.')[1],\n",
    "        'Title':data[15],\n",
    "        'SessionUserID':data[16]\n",
    "    }\n",
    "\n",
    "def parse_videos(x):\n",
    "    data = x.split(',')\n",
    "    return {\n",
    "        'TimeStamp':int(data[6]),\n",
    "        'Date':pd.to_datetime(int(data[6]),unit='s'),\n",
    "        'VideoID':int(data[10]),\n",
    "        'EventType':data[14].split('.')[0],\n",
    "        'EventSubType':data[14].split('.')[1],\n",
    "        'SessionUserID':data[15]\n",
    "    }\n",
    "\n",
    "def parse_forums(x):\n",
    "    data = x.split(',')\n",
    "    return {\n",
    "        'AccountUserID':data[3],\n",
    "        'TimeStamp':int(data[4]),\n",
    "        'Date':pd.to_datetime(int(data[4]),unit='s'),\n",
    "        'EventType':data[11].split('.')[0],\n",
    "        'EventSubType':data[11].split('.')[1]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to filter the string entries from previous dataset\n",
    "def filter_problems(x):\n",
    "    data = x.split(',')\n",
    "    return (\n",
    "        # Some problem event are quiz inside video, we decide to not consider these\n",
    "        (data[10]=='Assignment')\n",
    "        # We remove IDs of assignments that have been used by the teaching staff for testing the platform\n",
    "        and (not data[8] in ['1','2','3','4','5'])\n",
    "        # Discard assignments with no grade\n",
    "        and (not data[5] in ['None'])\n",
    "    )\n",
    "\n",
    "def filter_videos(x):\n",
    "    data = x.split(',')\n",
    "    return (\n",
    "        # We remove video that do not belong to the MOOC lectures or ar just Setup videos\n",
    "        (data[10] not in ['9','12','11','10','13','2','29','25','21','27','23'])\n",
    "        # Keeps only \"Play\" EnventSubStype for videos (makes difference between opening the page or starting the video)\n",
    "        and (data[14].split('.')[1] in ['Play','Load'])\n",
    "    )\n",
    "\n",
    "def filter_forums(x):\n",
    "    data = x.split(',')\n",
    "    return (\n",
    "        True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90325\n",
      "{'Date': Timestamp('2013-04-17 17:47:58'), 'SessionUserID': 'd8f79efa32a560b8a46ea2b12d9bed97c9e39b4b', 'Grade': 9.32999992371, 'EventSubType': 'Check', 'Title': 'Functional Sets / Functional Sets', 'ProblemType': 'Assignment', 'EventType': 'Problem', 'ProblemID': 6, 'TimeStamp': 1366220878} \n",
      "\n",
      "1168226\n",
      "{'Date': Timestamp('2013-06-28 03:53:58'), 'EventType': 'Video', 'EventSubType': 'Play', 'VideoID': 33, 'SessionUserID': 'ef64fb7b096008f7eaf8441684afdf99af9af54a', 'TimeStamp': 1372391638} \n",
      "\n",
      "297650\n",
      "{'Date': Timestamp('2013-08-11 20:50:35'), 'EventType': 'Forum', 'AccountUserID': '4108315', 'TimeStamp': 1376254235, 'EventSubType': 'Thread'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the map method to have more workable data\n",
    "rdd_problem_events_parsed = rdd_problem_events.filter(filter_problems).map(parse_problems)\n",
    "rdd_video_events_parsed = rdd_video_events.filter(filter_videos).map(parse_videos)\n",
    "rdd_forum_events_parsed = rdd_forum_events.filter(filter_forums).map(parse_forums)\n",
    "\n",
    "# Prints the count of elements along with the first element of each table\n",
    "print(rdd_problem_events_parsed.count())\n",
    "print(rdd_problem_events_parsed.first(),\"\\n\") \n",
    "print(rdd_video_events_parsed.count()) \n",
    "print(rdd_video_events_parsed.first(),\"\\n\") \n",
    "print(rdd_forum_events_parsed.count()) \n",
    "print(rdd_forum_events_parsed.first(),\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ForumUserID,AccountUserID,DataPackageID,UniqueRowID,TableName,Platform,SessionUserID \n",
      "\n",
      "297650\n",
      "{'Date': Timestamp('2013-03-27 20:32:02'), 'EventType': 'Forum', 'AccountUserID': '1932792', 'EventSubType': 'ThreadSubscribe', 'SessionUserID': 'a97848d806f0d88cf80dd154845693af40cf559f', 'TimeStamp': 1364416322}\n"
     ]
    }
   ],
   "source": [
    "# Handles bug with the forum events table having 'AccountUserID' instead of 'SessionUserID'\n",
    "# Using the table progfun-002_User_Hash_Mapping\n",
    "rdd_user_mapping = sc.textFile(\"../data/\"+config['session']+\"_User_Hash_Mapping.csv\")\n",
    "print(rdd_user_mapping.take(1)[0],\"\\n\")\n",
    "\n",
    "def f(x):\n",
    "    x[1][1]['SessionUserID']=x[1][0] \n",
    "    return x[1][1]\n",
    "\n",
    "rdd_forum_events_parsed = (rdd_user_mapping\n",
    "    # removes header\n",
    "    .filter(lambda x: not x.startswith(\"ForumUserID\"))\n",
    "    # maps to have the format (Key=AccountUserID,Value=SessionUserID)\n",
    "    .map(lambda x:(x.split(\",\")[1],x.split(\",\")[6]))\n",
    "    # join with rdd_forum_event to get format (map to have the (Key=AccountUserID,Value=(SessionUserID,EventObject)) format)\n",
    "    .join(rdd_forum_events_parsed\n",
    "        # map to have the (Key=AccountUserID,Value=EventObject) format\n",
    "        .map(lambda x: (x['AccountUserID'],x))\n",
    "    )\n",
    "    # Use the function f to update EventObject with the joined SessionUserID\n",
    "    .map(f)\n",
    ")\n",
    "\n",
    "print(rdd_forum_events_parsed.count())\n",
    "print(rdd_forum_events_parsed.take(1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1556201\n",
      "{'Date': Timestamp('2013-04-17 17:47:58'), 'SessionUserID': 'd8f79efa32a560b8a46ea2b12d9bed97c9e39b4b', 'Grade': 9.32999992371, 'EventSubType': 'Check', 'Title': 'Functional Sets / Functional Sets', 'ProblemType': 'Assignment', 'EventType': 'Problem', 'TimeStamp': 1366220878, 'ProblemID': 6}\n"
     ]
    }
   ],
   "source": [
    "# Concatenantes all three table into one big table\n",
    "rdd_events = (rdd_problem_events_parsed\n",
    "    .union(rdd_video_events_parsed)\n",
    "    .union(rdd_forum_events_parsed)\n",
    ")\n",
    "rdd_events.persist()\n",
    "print(rdd_events.count())\n",
    "print(rdd_events.take(1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing meta data\n",
    "The goal is to have the table linking VideoIDs to corresponding ProblemIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 'Functional Sets / Functional Sets')\n",
      "(7, 'Recursion / Recursion')\n",
      "(12, 'Object-Oriented Sets / Object-Oriented Sets')\n",
      "(14, 'Huffman Coding / Huffman Coding')\n",
      "(17, 'Anagrams / Anagrams')\n",
      "(20, 'Bloxorz / Bloxorz')\n"
     ]
    }
   ],
   "source": [
    "assignments = sorted((rdd_problem_events_parsed\n",
    "    .map(lambda x: (x['ProblemID'],x['Title']))\n",
    "    .distinct()\n",
    "    .collect()\n",
    "),key=(lambda x: x[0]))\n",
    "\n",
    "for item in assignments:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 'Test Lecture 2')\n",
      "(3, 'Lecture 1.2 - Elements of Programming (14:25)')\n",
      "(4, 'Lecture 1.3 - Evaluation Strategies and Termination (4:22)')\n",
      "(5, 'Lecture 1.4 - Conditionals and Value Definitions (8:49)')\n",
      "(6, 'Lecture 1.5 - Example: square roots with Newton<squote/>s method (11:25)')\n",
      "(7, 'Lecture 1.6 - Blocks and Lexical Scope (8:00)')\n",
      "(8, 'Lecture 1.1 - Programming Paradigms (14:32)')\n",
      "(9, 'Lecture 1.2')\n",
      "(10, 'Lecture 1.3')\n",
      "(11, 'Lecture 1.4')\n",
      "(12, 'Lecture 1.5')\n",
      "(13, 'Lecture 1.6')\n",
      "(21, 'Tools Setup for Windows (10:37)')\n",
      "(23, 'Tools Setup for Linux (12:24)')\n",
      "(25, 'Tools Setup for Mac OS X (12:17)')\n",
      "(27, 'Course Introduction (2:44)')\n",
      "(29, 'Tutorial: Working on the Programming Assignments (8:47)')\n",
      "(33, 'Lecture 1.7 - Tail Recursion (12:32)')\n",
      "(35, 'Lecture 2.1 - Higher-Order Functions (10:18)')\n",
      "(37, 'Lecture 2.2 - Currying (14:58)')\n",
      "(39, 'Lecture 2.3 - Example: Finding Fixed Points (10:46)')\n",
      "(41, 'Lecture 2.4 - Scala Syntax Summary (4:13)')\n",
      "(43, 'Lecture 2.5 - Functions and Data (11:50)')\n",
      "(47, 'Lecture 2.6 - More Fun With Rationals (15:08)')\n",
      "(49, 'Lecture 2.7 - Evaluation and Operators (16:25)')\n",
      "(51, 'Lecture 3.1 - Class Hierarchies (25:50)')\n",
      "(53, 'Lecture 3.2 - How Classes Are Organized (20:30)')\n",
      "(71, 'Lecture 4.7 - Lists (16:20)')\n",
      "(73, 'Lecture 5.1 - More Functions on Lists (13:04)')\n",
      "(75, 'Lecture 3.3 - Polymorphism (21:09)')\n",
      "(77, 'Lecture 4.2 - Objects Everywhere (19:07)')\n",
      "(79, 'Lecture 4.1 - Functions as Objects (8:04)')\n",
      "(81, 'Lecture 4.3 - Subtyping and Generics (15:02)')\n",
      "(83, 'Lecture 4.4 - Variance (Optional) (21:33)')\n",
      "(85, 'Lecture 4.5 - Decomposition (16:57)')\n",
      "(87, 'Lecture 4.6 - Pattern Matching (19:36)')\n",
      "(89, 'Lecture 5.2 - Pairs and Tuples (10:45)')\n",
      "(91, 'Lecture 5.3 - Implicit Parameters (11:08)')\n",
      "(93, 'Lecture 5.4 - Higher-Order List Functions (14:53)')\n",
      "(95, 'Lecture 5.5 - Reduction of Lists (15:35)')\n",
      "(97, 'Lecture 5.6 - Reasoning About Concat (13:00)')\n",
      "(101, 'Lecture 5.7 - A Larger Equational Proof on Lists (9:53)')\n",
      "(103, 'Lecture 6.1 - Other Collections (20:45)')\n",
      "(105, 'Lecture 6.2 - Combinatorial Search and For-Expressions (13:12)')\n",
      "(107, 'Lecture 6.3 - Combinatorial Search Example (16:54)')\n",
      "(109, 'Lecture 6.4 - Queries with For (7:50)')\n",
      "(111, 'Lecture 6.5 - Translation of For (11:23)')\n",
      "(113, 'Lecture 6.6 - Maps (22:39)')\n",
      "(115, 'Lecture 6.7 - Putting the Pieces Together (20:35)')\n",
      "(117, 'Lecture 7.1 - Structural Induction on Trees (15:10)')\n",
      "(119, 'Lecture 7.2 - Streams (12:12)')\n",
      "(121, 'Lecture 7.3 - Lazy Evaluation (11:38)')\n",
      "(123, 'Lecture 7.4 - Computing with Infinite Sequences (9:01)')\n",
      "(125, 'Lecture 7.5 - Case Study: the Water Pouring Problem (31:45)')\n",
      "(127, 'Lecture 7.6 - Course Conclusion (5:34)')\n"
     ]
    }
   ],
   "source": [
    "rdd_video_info = sc.textFile(\"../data/\"+config['session']+\"_Video_Info.csv\")\n",
    "videos = sorted((rdd_video_info\n",
    "    .map(lambda x: x.split(','))\n",
    "    .filter(lambda x: not x[1] in ['None', 'OpenTime'] )\n",
    "    .map(lambda x: (int(x[7]),x[2]))\n",
    "    .collect()\n",
    "),key=(lambda x: x[0]))\n",
    "          \n",
    "for video in videos:\n",
    "    print(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{6: [35, 37, 39, 41, 43, 47, 49],\n",
       " 7: [3, 4, 5, 6, 7, 8, 33],\n",
       " 12: [51, 53, 75],\n",
       " 14: [71, 81, 85, 79, 87, 77],\n",
       " 17: [109, 105, 115, 107, 103, 113, 111],\n",
       " 20: [123, 117, 125, 121, 127, 119]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Made by hand... so painful\n",
    "LECTURES_PER_PROBLEM = {\n",
    "    7: [3,4,5,6,7,8,33], # Lecture 1\n",
    "    6: [35,37,39,41,43,47,49], # Lecture 2\n",
    "    12: [51,53,75], # Lecture 3\n",
    "    14: [71,81,85,79,87,77], # Lecture 4\n",
    "    17: [109,105,115,107,103,113,111], # Lecture 6\n",
    "    20: [123,117,125,121,127,119] # Lecture 7\n",
    "}\n",
    "LECTURES_PER_PROBLEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 7,\n",
       " 4: 7,\n",
       " 5: 7,\n",
       " 6: 7,\n",
       " 7: 7,\n",
       " 8: 7,\n",
       " 33: 7,\n",
       " 35: 6,\n",
       " 37: 6,\n",
       " 39: 6,\n",
       " 41: 6,\n",
       " 43: 6,\n",
       " 47: 6,\n",
       " 49: 6,\n",
       " 51: 12,\n",
       " 53: 12,\n",
       " 71: 14,\n",
       " 75: 12,\n",
       " 77: 14,\n",
       " 79: 14,\n",
       " 81: 14,\n",
       " 85: 14,\n",
       " 87: 14,\n",
       " 103: 17,\n",
       " 105: 17,\n",
       " 107: 17,\n",
       " 109: 17,\n",
       " 111: 17,\n",
       " 113: 17,\n",
       " 115: 17,\n",
       " 117: 20,\n",
       " 119: 20,\n",
       " 121: 20,\n",
       " 123: 20,\n",
       " 125: 20,\n",
       " 127: 20}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROBLEM_PER_LECTURE = {}\n",
    "for pb in LECTURES_PER_PROBLEM.keys():\n",
    "    for lc in LECTURES_PER_PROBLEM[pb]:\n",
    "        PROBLEM_PER_LECTURE[lc]=pb\n",
    "PROBLEM_PER_LECTURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping & Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of students: 24626\n"
     ]
    }
   ],
   "source": [
    "# uses the function groupByKey on our events with the key 'SessionStudentID'\n",
    "rdd_events_by_students = (rdd_events\n",
    "    .map(lambda x: (x['SessionUserID'],x))\n",
    "    # Groups the list of event by student\n",
    "    .groupByKey()\n",
    "    # Sorts each student sequence by the timestamp\n",
    "    .map(lambda x: (x[0],sorted(x[1], key=(lambda event: event['TimeStamp']))))\n",
    ")\n",
    "rdd_events_by_students.persist()\n",
    "print(\"Number of students: %d\" % rdd_events_by_students.count()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to have a friendly way to print the events\n",
    "def eventToString(event):\n",
    "    return {\n",
    "        \"Problem\": lambda x: \"(P \"+str(event['ProblemID'])+\")\",\n",
    "        \"Video\": lambda x: \"(V \"+str(event['VideoID'])+\")\",\n",
    "        \"Forum\": lambda x: \"(F)\"\n",
    "    }[event['EventType']](event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEST_STUDENTS={\n",
    "    '6ea6949ca133acede360d3573f9d1168b3d70b51':'very good student',\n",
    "    '5046ee71bc77983a0753d6e5ba98f5f1c685072e':'watched 4 videos',\n",
    "    '3b305429f93de02637949578a5f9e23f13eb0726':'did two problems',\n",
    "    '9625e050ef2326c12632c51aea7b5e49a20d6fc7':'good student',\n",
    "    '865981d5b40a693bafbadae4b1df769be03a25c3':'watched 8 videos',\n",
    "    '67cdae6073d1089b695e2a615a01187586ad7ba6':'normal student',\n",
    "    'fb4c81b3df430d1f0fbb8d0ca3e470ac6bf92a2f':'the very best student',\n",
    "    '295b3496278626b6d337812b1882b756336fd633':'whatdup with this guy?'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 6ea6949ca133acede360d3573f9d1168b3d70b51 : very good student\n",
      "(V 8) (V 3) (V 3) (V 4) (V 5) (V 5) (V 6) (V 6) (V 6) (V 6) (V 6) (V 6) (V 7) (V 7) (V 7) (V 33) (V 33) (V 33) (P 7) (V 35) (V 35) (V 35) (V 35) (V 35) (V 35) (V 35) (V 35) (V 35) (V 35) (V 35) (V 35) (V 37) (V 37) (V 37) (V 37) (V 37) (V 37) (V 37) (V 37) (V 39) (V 39) (V 39) (V 41) (V 41) (V 43) (V 47) (V 49) (P 6) (P 6) (P 6) (V 51) (V 51) (V 51) (V 53) (V 53) (V 53) (V 75) (V 75) (V 75) (V 75) (P 12) (V 79) (V 77) (V 81) (V 83) (V 85) (V 87) (V 71) (P 14) (P 14) (V 73) (V 89) (V 91) (V 93) (V 95) (V 95) (V 97) (V 101) (P 14) (P 14) (P 14) (P 14) (P 14) (P 14) (P 14) (V 103) (V 105) (V 107) (V 109) (V 111) (V 113) (V 115) (F) (F) (F) (F) (F) (F) (F) (F) (P 17) (V 109) (V 111) (V 113) (V 107) (V 117) (V 119) (V 121) (V 123) (V 125) (V 127) (V 125) (F) (F) (P 20) (F) (F) (V 125) (V 125) (P 20) (F) \n",
      "\n",
      " 295b3496278626b6d337812b1882b756336fd633 : whatdup with this guy?\n",
      "(V 8) (V 3) (V 3) (V 4) (V 5) (V 5) (V 6) (V 7) (V 33) (P 7) (V 35) (V 37) (V 39) (V 41) (V 43) (V 47) (V 47) (V 49) (P 6) (V 51) (V 51) (V 51) (V 51) (V 51) (V 53) (V 75) (P 12) (P 12) (V 79) (V 79) (V 77) (V 81) (V 83) (V 83) (V 85) (V 87) (V 71) (P 14) (P 14) (P 14) (P 14) (V 73) (V 73) (V 73) (V 73) (V 73) (V 73) (V 73) (V 89) (V 89) (V 89) (V 91) (V 91) (V 91) (V 93) (V 93) (V 93) (V 93) (V 93) (V 93) (V 93) (V 93) (V 93) (V 95) (V 95) (V 95) (V 95) (V 97) (V 97) (V 97) (V 101) (V 101) (V 101) (V 103) (V 103) (V 103) (V 105) (V 105) (V 105) (V 107) (V 107) (V 107) (V 107) (V 109) (V 109) (V 109) (V 109) (V 111) (V 111) (V 111) (V 113) (V 113) (V 113) (V 113) (V 113) (V 113) (V 113) (V 115) (V 115) (V 121) (P 17) (P 17) (V 117) (V 117) (V 117) (V 117) (V 119) (V 119) (V 119) (V 119) (V 119) (V 119) (V 119) (V 119) (V 119) (V 119) (V 119) (V 119) (V 119) (V 119) (V 119) (V 119) (V 119) (V 119) (V 119) (V 119) (V 121) (V 121) (V 121) (V 121) (V 121) (V 121) (V 123) (V 123) (V 123) (V 125) (V 125) (V 125) (V 125) (P 20) (V 127) \n",
      "\n",
      " 67cdae6073d1089b695e2a615a01187586ad7ba6 : normal student\n",
      "(V 8) (V 6) (V 7) (V 6) (V 3) (V 4) (V 5) (V 33) (V 6) (V 33) (P 7) (V 35) (V 37) (V 41) (P 6) (V 43) (V 47) (V 49) (V 39) (F) (F) (V 51) (V 53) (V 75) (P 12) (P 12) (V 79) (V 77) (V 81) (V 83) (V 83) (V 85) (V 87) (V 87) (V 71) (P 14) (P 12) (P 12) (P 17) (P 20) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) \n",
      "\n",
      " 5046ee71bc77983a0753d6e5ba98f5f1c685072e : watched 4 videos\n",
      "(V 8) (V 8) (V 3) (V 3) (V 4) \n",
      "\n",
      " 865981d5b40a693bafbadae4b1df769be03a25c3 : watched 8 videos\n",
      "(V 8) (V 3) (V 4) (V 5) (V 6) (V 7) (V 7) (V 33) (V 35) (V 35) (V 35) (V 35) \n",
      "\n",
      " 9625e050ef2326c12632c51aea7b5e49a20d6fc7 : good student\n",
      "(V 8) (V 3) (V 3) (V 4) (V 5) (V 6) (V 7) (V 7) (V 33) (V 35) (V 35) (V 35) (V 35) (V 35) (V 35) (V 35) (V 35) (V 37) (V 37) (P 7) (V 37) (V 39) (V 39) (V 41) (V 43) (V 47) (V 47) (V 37) (V 47) (V 49) (P 6) (V 51) (V 53) (V 75) (P 12) (V 79) (V 79) (V 77) (V 81) (V 81) (V 81) (V 83) (V 85) (V 87) (V 71) (P 14) (P 14) (P 14) (V 73) (V 73) (V 73) (V 73) (V 73) (V 73) (V 91) (V 93) (V 93) (V 95) (V 95) (V 101) (V 97) (V 97) (V 103) (V 105) (V 107) (V 109) (V 111) (V 113) (V 113) (V 113) (V 115) (V 109) (V 105) (F) (F) (F) (F) (F) (F) (P 17) (V 117) (V 117) (V 119) (V 121) (V 123) (V 123) (V 123) (V 125) (V 127) (V 125) (V 125) (F) (F) (F) (F) (F) (F) (P 20) (P 20) (F) (F) (F) (F) \n",
      "\n",
      " 3b305429f93de02637949578a5f9e23f13eb0726 : did two problems\n",
      "(P 7) (P 6) \n",
      "\n",
      " fb4c81b3df430d1f0fbb8d0ca3e470ac6bf92a2f : the very best student\n",
      "(V 8) (V 8) (V 3) (V 4) (V 5) (V 4) (V 6) (V 7) (V 7) (V 7) (V 33) (P 7) (V 35) (V 35) (V 35) (V 35) (V 37) (V 37) (V 37) (V 39) (V 39) (V 39) (V 39) (V 41) (V 41) (V 43) (V 43) (V 47) (V 47) (V 47) (V 47) (V 47) (V 47) (V 49) (V 49) (F) (F) (F) (F) (F) (P 7) (P 7) (P 7) (F) (P 7) (P 7) (P 6) (P 6) (V 51) (V 51) (V 53) (V 53) (V 53) (V 53) (V 51) (V 51) (V 51) (V 51) (V 51) (V 51) (V 51) (V 51) (V 53) (V 53) (V 75) (V 75) (P 12) (P 12) (P 12) (P 12) (P 12) (P 12) (F) (F) (F) (F) (V 79) (V 79) (V 77) (V 81) (V 83) (V 85) (V 87) (V 71) (V 71) (P 12) (P 12) (P 12) (P 12) (V 73) (V 73) (V 73) (P 14) (P 14) (P 14) (P 14) (V 73) (V 89) (V 91) (V 91) (V 93) (V 95) (V 97) (V 101) (P 14) (V 103) (V 103) (V 105) (V 105) (V 107) (V 107) (V 107) (V 107) (V 107) (V 107) (V 109) (V 109) (V 111) (V 111) (V 111) (V 111) (V 113) (V 113) (V 115) (V 115) (V 117) (V 119) (V 121) (V 121) (V 123) (P 17) (P 17) (V 125) (V 127) (P 17) (P 20) (P 20) (P 20) (P 17) (P 17) (P 17) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (P 7) (F) (F) (F) (F) (F) (F) (F) (F) (F) (P 7) (F) (F) (F) (F) (F) (F) (F) (F) (F) (F) (P 17) (P 17) \n"
     ]
    }
   ],
   "source": [
    "# Looks at the data for a few students\n",
    "for studentID,events in rdd_events_by_students.filter(lambda x: x[0] in TEST_STUDENTS.keys()).collect():\n",
    "    print('\\n',studentID,':',TEST_STUDENTS[studentID])\n",
    "    for event in events:\n",
    "        print(eventToString(event), end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting learning patterns\n",
    "The goal is to extract what a students does between a failed attempt at an assignment and a successful attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The goal is to extract the list corresponding to each Problem\n",
    "def extractPatterns(events):\n",
    "    patterns = {}\n",
    "    for problemID in LECTURES_PER_PROBLEM.keys():\n",
    "        patterns[problemID]=[]\n",
    "    # for each problem measure first and last entry\n",
    "    for event in events:\n",
    "        if event['EventType']=='Problem':\n",
    "            patterns[event['ProblemID']].append(event)\n",
    "        if event['EventType']=='Video':\n",
    "            if event['VideoID'] not in PROBLEM_PER_LECTURE.keys():\n",
    "                print('WRONG VIDEO ID',event)\n",
    "            else:\n",
    "                patterns[PROBLEM_PER_LECTURE[event['VideoID']]].append(event)\n",
    "    return patterns\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The goal is to concatenate small sub events into one big event\n",
    "# For example Play/Pause/Play/Pause/Play/Pause on a video counts as only a Video event\n",
    "def eventConcat(events):\n",
    "    if len(events)<2:\n",
    "        return events\n",
    "    res=[events[0]]\n",
    "    for event in events[1:-1]:\n",
    "        if (\n",
    "            event['EventType']!=res[-1]['EventType'] \n",
    "            or event.get('VideoID',None)!=res[-1].get('VideoID',None)\n",
    "        ):\n",
    "            res.append(event)\n",
    "    res.append(events[-1])\n",
    "    return res\n",
    "\n",
    "def patternsConcat(patterns):\n",
    "    res = {}\n",
    "    for pattern in patterns.keys():\n",
    "        res[pattern]=eventConcat(patterns[pattern])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6ea6949ca133acede360d3573f9d1168b3d70b51 very good student\n",
      "6\t>>>  (V 35) (V 37) (V 39) (V 41) (V 43) (V 47) (V 49) (P 6) (P 6) \n",
      "7\t>>>  (V 8) (V 3) (V 4) (V 5) (V 6) (V 7) (V 33) (P 7) \n",
      "12\t>>>  (V 51) (V 53) (V 75) (P 12) \n",
      "14\t>>>  (V 79) (V 77) (V 81) (V 85) (V 87) (V 71) (P 14) (P 14) \n",
      "17\t>>>  (V 103) (V 105) (V 107) (V 109) (V 111) (V 113) (V 115) (P 17) (V 109) (V 111) (V 113) (V 107) \n",
      "20\t>>>  (V 117) (V 119) (V 121) (V 123) (V 125) (V 127) (V 125) (P 20) (V 125) (P 20) \n",
      "\n",
      "295b3496278626b6d337812b1882b756336fd633 whatdup with this guy?\n",
      "6\t>>>  (V 35) (V 37) (V 39) (V 41) (V 43) (V 47) (V 49) (P 6) \n",
      "7\t>>>  (V 8) (V 3) (V 4) (V 5) (V 6) (V 7) (V 33) (P 7) \n",
      "12\t>>>  (V 51) (V 53) (V 75) (P 12) (P 12) \n",
      "14\t>>>  (V 79) (V 77) (V 81) (V 85) (V 87) (V 71) (P 14) (P 14) \n",
      "17\t>>>  (V 103) (V 105) (V 107) (V 109) (V 111) (V 113) (V 115) (P 17) (P 17) \n",
      "20\t>>>  (V 121) (V 117) (V 119) (V 121) (V 123) (V 125) (P 20) (V 127) \n",
      "\n",
      "67cdae6073d1089b695e2a615a01187586ad7ba6 normal student\n",
      "6\t>>>  (V 35) (V 37) (V 41) (P 6) (V 43) (V 47) (V 49) (V 39) \n",
      "7\t>>>  (V 8) (V 6) (V 7) (V 6) (V 3) (V 4) (V 5) (V 33) (V 6) (V 33) (P 7) \n",
      "12\t>>>  (V 51) (V 53) (V 75) (P 12) (P 12) \n",
      "14\t>>>  (V 79) (V 77) (V 81) (V 85) (V 87) (V 71) (P 14) \n",
      "17\t>>>  (P 17) \n",
      "20\t>>>  (P 20) \n",
      "\n",
      "5046ee71bc77983a0753d6e5ba98f5f1c685072e watched 4 videos\n",
      "6\t>>>  \n",
      "7\t>>>  (V 8) (V 3) (V 4) \n",
      "12\t>>>  \n",
      "14\t>>>  \n",
      "17\t>>>  \n",
      "20\t>>>  \n",
      "\n",
      "865981d5b40a693bafbadae4b1df769be03a25c3 watched 8 videos\n",
      "6\t>>>  (V 35) (V 35) \n",
      "7\t>>>  (V 8) (V 3) (V 4) (V 5) (V 6) (V 7) (V 33) \n",
      "12\t>>>  \n",
      "14\t>>>  \n",
      "17\t>>>  \n",
      "20\t>>>  \n",
      "\n",
      "9625e050ef2326c12632c51aea7b5e49a20d6fc7 good student\n",
      "6\t>>>  (V 35) (V 37) (V 39) (V 41) (V 43) (V 47) (V 37) (V 47) (V 49) (P 6) \n",
      "7\t>>>  (V 8) (V 3) (V 4) (V 5) (V 6) (V 7) (V 33) (P 7) \n",
      "12\t>>>  (V 51) (V 53) (V 75) (P 12) \n",
      "14\t>>>  (V 79) (V 77) (V 81) (V 85) (V 87) (V 71) (P 14) (P 14) \n",
      "17\t>>>  (V 103) (V 105) (V 107) (V 109) (V 111) (V 113) (V 115) (V 109) (V 105) (P 17) \n",
      "20\t>>>  (V 117) (V 119) (V 121) (V 123) (V 125) (V 127) (V 125) (P 20) (P 20) \n",
      "\n",
      "3b305429f93de02637949578a5f9e23f13eb0726 did two problems\n",
      "6\t>>>  (P 6) \n",
      "7\t>>>  (P 7) \n",
      "12\t>>>  \n",
      "14\t>>>  \n",
      "17\t>>>  \n",
      "20\t>>>  \n",
      "\n",
      "fb4c81b3df430d1f0fbb8d0ca3e470ac6bf92a2f the very best student\n",
      "6\t>>>  (V 35) (V 37) (V 39) (V 41) (V 43) (V 47) (V 49) (P 6) (P 6) \n",
      "7\t>>>  (V 8) (V 3) (V 4) (V 5) (V 4) (V 6) (V 7) (V 33) (P 7) (P 7) \n",
      "12\t>>>  (V 51) (V 53) (V 51) (V 53) (V 75) (P 12) (P 12) \n",
      "14\t>>>  (V 79) (V 77) (V 81) (V 85) (V 87) (V 71) (P 14) (P 14) \n",
      "17\t>>>  (V 103) (V 105) (V 107) (V 109) (V 111) (V 113) (V 115) (P 17) (P 17) \n",
      "20\t>>>  (V 117) (V 119) (V 121) (V 123) (V 125) (V 127) (P 20) (P 20) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd_events_by_students_extract = (rdd_events_by_students\n",
    "    .map(lambda x: (x[0],extractPatterns(x[1])))\n",
    "    .map(lambda x: (x[0],patternsConcat(x[1])))\n",
    ")\n",
    "\n",
    "rdd_events_by_students_extract.persist()\n",
    "\n",
    "# Looks at the data for a student, now that it's sorted by TimeStamps\n",
    "def displayStudentsPatterns(rdd,students):\n",
    "    for studentID,patterns in rdd.filter(lambda x:x[0] in students.keys()).collect():\n",
    "        print(studentID, students[studentID])\n",
    "        for pb in sorted(patterns.keys()):\n",
    "            print(pb,end='\\t>>>  ')\n",
    "            for event in patterns[pb]:\n",
    "                print(eventToString(event), end=\" \")\n",
    "            print()\n",
    "        print()\n",
    "\n",
    "displayStudentsPatterns(rdd_events_by_students_extract,TEST_STUDENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('PVPVVVVVVVVV', <pyspark.resultiterable.ResultIterable object at 0x111de79e8>)\n"
     ]
    }
   ],
   "source": [
    "def eventToLetter(event):\n",
    "    return {\n",
    "        \"Problem\": lambda x:\"P\",\n",
    "        \"Video\": lambda x:\"V\",\n",
    "        \"Forum\": lambda x:\"F\",\n",
    "    }[event['EventType']](event)\n",
    "\n",
    "def patternToSimpleString(pattern):\n",
    "    return \"\".join([eventToLetter(event) for event in pattern])\n",
    "\n",
    "# Uses the method flatMap on the students patterns table to have a table of all the patterns\n",
    "rdd_patterns = (rdd_events_by_students_extract\n",
    "    .flatMap(lambda x: [\n",
    "        ((patternToSimpleString(x[1][pb]),pb),1) for pb in x[1].keys() \n",
    "    ])\n",
    "    .filter(lambda x: 'P' in x[0][0])\n",
    "    .reduceByKey(lambda a,b: a+b)\n",
    "    .map(lambda x: (x[0][0],{'problem':x[0][1],'count':x[1]}))\n",
    "    .groupByKey()\n",
    ")\n",
    "\n",
    "rdd_patterns.persist()\n",
    "\n",
    "print(rdd_patterns.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5991 \t P\n",
      "\t {'problem': 17, 'count': 1147}\n",
      "\t {'problem': 7, 'count': 1104}\n",
      "\t {'problem': 6, 'count': 1043}\n",
      "\t {'problem': 20, 'count': 988}\n",
      "\t {'problem': 12, 'count': 903}\n",
      "\t {'problem': 14, 'count': 806}\n",
      "5388 \t VVVVVVVP\n",
      "\t {'problem': 7, 'count': 2674}\n",
      "\t {'problem': 6, 'count': 1428}\n",
      "\t {'problem': 17, 'count': 731}\n",
      "\t {'problem': 20, 'count': 376}\n",
      "\t {'problem': 14, 'count': 131}\n",
      "\t {'problem': 12, 'count': 48}\n",
      "4628 \t PP\n",
      "\t {'problem': 14, 'count': 935}\n",
      "\t {'problem': 6, 'count': 901}\n",
      "\t {'problem': 12, 'count': 901}\n",
      "\t {'problem': 17, 'count': 650}\n",
      "\t {'problem': 20, 'count': 639}\n",
      "\t {'problem': 7, 'count': 602}\n",
      "3508 \t VVVVVVVPP\n",
      "\t {'problem': 7, 'count': 1254}\n",
      "\t {'problem': 6, 'count': 1174}\n",
      "\t {'problem': 20, 'count': 454}\n",
      "\t {'problem': 17, 'count': 384}\n",
      "\t {'problem': 14, 'count': 197}\n",
      "\t {'problem': 12, 'count': 45}\n",
      "1896 \t VVVVVVPP\n",
      "\t {'problem': 14, 'count': 1225}\n",
      "\t {'problem': 20, 'count': 428}\n",
      "\t {'problem': 12, 'count': 118}\n",
      "\t {'problem': 7, 'count': 47}\n",
      "\t {'problem': 6, 'count': 44}\n",
      "\t {'problem': 17, 'count': 34}\n",
      "1713 \t VVVP\n",
      "\t {'problem': 12, 'count': 1377}\n",
      "\t {'problem': 7, 'count': 95}\n",
      "\t {'problem': 6, 'count': 84}\n",
      "\t {'problem': 17, 'count': 59}\n",
      "\t {'problem': 20, 'count': 52}\n",
      "\t {'problem': 14, 'count': 46}\n",
      "1627 \t VVVVVVP\n",
      "\t {'problem': 14, 'count': 816}\n",
      "\t {'problem': 20, 'count': 494}\n",
      "\t {'problem': 12, 'count': 107}\n",
      "\t {'problem': 7, 'count': 91}\n",
      "\t {'problem': 6, 'count': 61}\n",
      "\t {'problem': 17, 'count': 58}\n",
      "1516 \t VVVPP\n",
      "\t {'problem': 12, 'count': 1246}\n",
      "\t {'problem': 14, 'count': 78}\n",
      "\t {'problem': 6, 'count': 62}\n",
      "\t {'problem': 20, 'count': 48}\n",
      "\t {'problem': 7, 'count': 41}\n",
      "\t {'problem': 17, 'count': 41}\n",
      "1221 \t VP\n",
      "\t {'problem': 7, 'count': 345}\n",
      "\t {'problem': 12, 'count': 273}\n",
      "\t {'problem': 6, 'count': 191}\n",
      "\t {'problem': 17, 'count': 151}\n",
      "\t {'problem': 20, 'count': 139}\n",
      "\t {'problem': 14, 'count': 122}\n",
      "1123 \t VVVVVVVVVP\n",
      "\t {'problem': 7, 'count': 508}\n",
      "\t {'problem': 6, 'count': 282}\n",
      "\t {'problem': 17, 'count': 211}\n",
      "\t {'problem': 20, 'count': 61}\n",
      "\t {'problem': 14, 'count': 43}\n",
      "\t {'problem': 12, 'count': 18}\n",
      "1071 \t VPP\n",
      "\t {'problem': 12, 'count': 243}\n",
      "\t {'problem': 7, 'count': 190}\n",
      "\t {'problem': 6, 'count': 183}\n",
      "\t {'problem': 14, 'count': 172}\n",
      "\t {'problem': 20, 'count': 166}\n",
      "\t {'problem': 17, 'count': 117}\n",
      "966 \t VVVVVVVVP\n",
      "\t {'problem': 7, 'count': 277}\n",
      "\t {'problem': 6, 'count': 235}\n",
      "\t {'problem': 17, 'count': 198}\n",
      "\t {'problem': 14, 'count': 136}\n",
      "\t {'problem': 20, 'count': 94}\n",
      "\t {'problem': 12, 'count': 26}\n",
      "911 \t VVVVVVVVPP\n",
      "\t {'problem': 14, 'count': 273}\n",
      "\t {'problem': 6, 'count': 220}\n",
      "\t {'problem': 7, 'count': 163}\n",
      "\t {'problem': 20, 'count': 118}\n",
      "\t {'problem': 17, 'count': 118}\n",
      "\t {'problem': 12, 'count': 19}\n",
      "817 \t VVVVVVVVVPP\n",
      "\t {'problem': 6, 'count': 279}\n",
      "\t {'problem': 7, 'count': 257}\n",
      "\t {'problem': 17, 'count': 113}\n",
      "\t {'problem': 14, 'count': 80}\n",
      "\t {'problem': 20, 'count': 75}\n",
      "\t {'problem': 12, 'count': 13}\n",
      "606 \t VVVVP\n",
      "\t {'problem': 12, 'count': 320}\n",
      "\t {'problem': 7, 'count': 102}\n",
      "\t {'problem': 6, 'count': 56}\n",
      "\t {'problem': 20, 'count': 50}\n",
      "\t {'problem': 17, 'count': 46}\n",
      "\t {'problem': 14, 'count': 32}\n",
      "571 \t VVVVPP\n",
      "\t {'problem': 12, 'count': 334}\n",
      "\t {'problem': 7, 'count': 54}\n",
      "\t {'problem': 6, 'count': 51}\n",
      "\t {'problem': 20, 'count': 49}\n",
      "\t {'problem': 14, 'count': 46}\n",
      "\t {'problem': 17, 'count': 37}\n",
      "564 \t VVVVVVVPV\n",
      "\t {'problem': 7, 'count': 283}\n",
      "\t {'problem': 6, 'count': 156}\n",
      "\t {'problem': 17, 'count': 51}\n",
      "\t {'problem': 20, 'count': 44}\n",
      "\t {'problem': 14, 'count': 25}\n",
      "\t {'problem': 12, 'count': 5}\n",
      "543 \t VVP\n",
      "\t {'problem': 7, 'count': 160}\n",
      "\t {'problem': 12, 'count': 106}\n",
      "\t {'problem': 6, 'count': 96}\n",
      "\t {'problem': 17, 'count': 68}\n",
      "\t {'problem': 14, 'count': 57}\n",
      "\t {'problem': 20, 'count': 56}\n",
      "517 \t VVVVVVVPVV\n",
      "\t {'problem': 7, 'count': 291}\n",
      "\t {'problem': 6, 'count': 131}\n",
      "\t {'problem': 17, 'count': 29}\n",
      "\t {'problem': 20, 'count': 29}\n",
      "\t {'problem': 14, 'count': 27}\n",
      "\t {'problem': 12, 'count': 10}\n",
      "511 \t VVVVVP\n",
      "\t {'problem': 12, 'count': 209}\n",
      "\t {'problem': 20, 'count': 85}\n",
      "\t {'problem': 7, 'count': 74}\n",
      "\t {'problem': 6, 'count': 63}\n",
      "\t {'problem': 14, 'count': 42}\n",
      "\t {'problem': 17, 'count': 38}\n"
     ]
    }
   ],
   "source": [
    "def sumCount(counts):\n",
    "    return sum([i['count'] for i in counts])\n",
    "\n",
    "# filters and shows the most common patterns \n",
    "for pattern in sorted(rdd_patterns.collect(), key=(lambda x: sumCount(x[1])), reverse=True)[:20]:\n",
    "    print(sumCount(pattern[1]),'\\t',pattern[0])\n",
    "    for example in sorted(pattern[1],key=(lambda x: x['count']), reverse=True):\n",
    "        print(\"\\t\",example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grade \t P \t V \t F\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 51.0 failed 1 times, most recent failure: Lost task 0.0 in stage 51.0 (TID 305, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/louisfaucon/Documents/spark-2.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/Users/louisfaucon/Documents/spark-2.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/louisfaucon/Documents/spark-2.0.1-bin-hadoop2.7/python/pyspark/rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/louisfaucon/Documents/spark-2.0.1-bin-hadoop2.7/python/pyspark/rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/louisfaucon/Documents/spark-2.0.1-bin-hadoop2.7/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/Users/louisfaucon/Documents/spark-2.0.1-bin-hadoop2.7/python/pyspark/rdd.py\", line 1792, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/Users/louisfaucon/Documents/spark-2.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/shuffle.py\", line 236, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-26-b4df3ab89f10>\", line 3, in <lambda>\n  File \"<ipython-input-26-b4df3ab89f10>\", line 3, in <listcomp>\nKeyError: 'from'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1890)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1903)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1916)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/louisfaucon/Documents/spark-2.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/Users/louisfaucon/Documents/spark-2.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/louisfaucon/Documents/spark-2.0.1-bin-hadoop2.7/python/pyspark/rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/louisfaucon/Documents/spark-2.0.1-bin-hadoop2.7/python/pyspark/rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/louisfaucon/Documents/spark-2.0.1-bin-hadoop2.7/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/Users/louisfaucon/Documents/spark-2.0.1-bin-hadoop2.7/python/pyspark/rdd.py\", line 1792, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/Users/louisfaucon/Documents/spark-2.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/shuffle.py\", line 236, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-26-b4df3ab89f10>\", line 3, in <lambda>\n  File \"<ipython-input-26-b4df3ab89f10>\", line 3, in <listcomp>\nKeyError: 'from'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-b4df3ab89f10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Grade'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'P'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'V'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'F'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrdd_onestep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mcounts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/louisfaucon/Documents/spark-2.0.1-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/louisfaucon/Documents/spark-2.0.1-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/louisfaucon/anaconda/lib/python3.5/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/louisfaucon/anaconda/lib/python3.5/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 51.0 failed 1 times, most recent failure: Lost task 0.0 in stage 51.0 (TID 305, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/louisfaucon/Documents/spark-2.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/Users/louisfaucon/Documents/spark-2.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/louisfaucon/Documents/spark-2.0.1-bin-hadoop2.7/python/pyspark/rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/louisfaucon/Documents/spark-2.0.1-bin-hadoop2.7/python/pyspark/rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/louisfaucon/Documents/spark-2.0.1-bin-hadoop2.7/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/Users/louisfaucon/Documents/spark-2.0.1-bin-hadoop2.7/python/pyspark/rdd.py\", line 1792, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/Users/louisfaucon/Documents/spark-2.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/shuffle.py\", line 236, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-26-b4df3ab89f10>\", line 3, in <lambda>\n  File \"<ipython-input-26-b4df3ab89f10>\", line 3, in <listcomp>\nKeyError: 'from'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1890)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1903)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1916)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/louisfaucon/Documents/spark-2.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/Users/louisfaucon/Documents/spark-2.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/louisfaucon/Documents/spark-2.0.1-bin-hadoop2.7/python/pyspark/rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/louisfaucon/Documents/spark-2.0.1-bin-hadoop2.7/python/pyspark/rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/louisfaucon/Documents/spark-2.0.1-bin-hadoop2.7/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/Users/louisfaucon/Documents/spark-2.0.1-bin-hadoop2.7/python/pyspark/rdd.py\", line 1792, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/Users/louisfaucon/Documents/spark-2.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/shuffle.py\", line 236, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-26-b4df3ab89f10>\", line 3, in <lambda>\n  File \"<ipython-input-26-b4df3ab89f10>\", line 3, in <listcomp>\nKeyError: 'from'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "rdd_onestep = (rdd_patterns\n",
    "    .filter(lambda x: len(x[0])>1)\n",
    "    .flatMap(lambda x: [((int(grade['from']),x[0][1:2]),1) for grade in x[1]])\n",
    "    .reduceByKey(lambda a,b:a+b)\n",
    "    .map(lambda x: (x[0][0],(x[0][1],x[1])))\n",
    "    .groupByKey()\n",
    ")\n",
    "\n",
    "print('Grade','\\t','P','\\t','V','\\t','F')\n",
    "print()\n",
    "for item in rdd_onestep.take(10):\n",
    "    print(item[0], end='\\t')\n",
    "    counts={}\n",
    "    for event,count in list(item[1]):\n",
    "        counts[event]=count\n",
    "    print('{0:.2f}'.format(counts['P']/sum(counts.values())),end='\\t')\n",
    "    print('{0:.2f}'.format(counts['V']/sum(counts.values())),end='\\t')\n",
    "    print('{0:.2f}'.format(counts['F']/sum(counts.values())),end='\\t')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
