{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for Sankey Diagram with D3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import utils\n",
    "import pandas as pd\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local[*]\", \"ADA\")\n",
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile('data/spark/preprocessed/').map(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({7: 9721, 21: 8551, 22: 4731, 23: 6128, 24: 7310, 25: 5350})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display number of records per problem id\n",
    "Counter(rdd.map(lambda x: x['ProblemID']).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PROBLEM_ID = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_data(item):\n",
    "    original_pattern = Pattern = item['Pattern']\n",
    "    new_pattern = 'S,'\n",
    "    \n",
    "    for pattern_item in original_pattern:\n",
    "        if (pattern_item['EventType'] == 'Video' and pattern_item['EventSubType'] == 'Load'):\n",
    "            pattern_item = 'V' + str(pattern_item['VideoID']) + ','\n",
    "            # Do not write if loop\n",
    "            if pattern_item not in new_pattern[-5:]:\n",
    "                new_pattern += pattern_item\n",
    "        elif pattern_item['EventType'] == 'Problem':\n",
    "            new_pattern += 'A' + str(new_pattern.count('A') + 1) + ','\n",
    "            \n",
    "    return({\n",
    "        \"StudentID\": item['StudentID'],\n",
    "        \"ProblemID\": item['ProblemID'],\n",
    "        \"Pattern\": new_pattern[:-1],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_final_patterns(rdd):\n",
    "    # Filter by problemID and clean pattern\n",
    "    rdd_processed = rdd.filter(lambda x: x['ProblemID'] == PROBLEM_ID).map(process_data)\n",
    "    # Counts the occurences of the patterns\n",
    "    patterns_with_counts = dict(Counter(rdd_processed.map(lambda x: x['Pattern']).collect()))\n",
    "    # Remove the patterns with less than 50 occurences\n",
    "    final_patterns = {k: v for k, v in patterns_with_counts.items() if v > 30}\n",
    "    return final_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_patterns = get_final_patterns(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S,A1': 1319,\n",
       " 'S,A1,A2': 338,\n",
       " 'S,A1,A2,A3': 117,\n",
       " 'S,A1,A2,A3,A4': 35,\n",
       " 'S,V103,A1': 93,\n",
       " 'S,V103,V105,A1': 46,\n",
       " 'S,V103,V105,V107,A1': 36,\n",
       " 'S,V103,V105,V107,V109,V111,A1': 37,\n",
       " 'S,V103,V105,V107,V109,V111,V113,A1': 46,\n",
       " 'S,V103,V105,V107,V109,V111,V113,V115,A1': 708,\n",
       " 'S,V103,V105,V107,V109,V111,V113,V115,A1,A2': 221,\n",
       " 'S,V103,V105,V107,V109,V111,V113,V115,A1,A2,A3': 46,\n",
       " 'S,V103,V105,V107,V109,V111,V113,V115,V113,A1': 45,\n",
       " 'S,V103,V105,V107,V109,V111,V113,V115,V113,V115,A1': 33,\n",
       " 'S,V115,A1': 32}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_json_file(final_patterns):\n",
    "    json_file = {\"sankey\": OrderedDict()}\n",
    "    json_file['sankey']['nodes'] = []\n",
    "    json_file['sankey']['links'] = []\n",
    "    \n",
    "    for pattern, count in final_patterns.items():\n",
    "        pattern_elements = pattern.split(',')\n",
    "        \n",
    "        for i in range(len(pattern_elements) - 1):\n",
    "            n_nodes = len(json_file['sankey']['nodes'])\n",
    "            event_start = pattern_elements[i]\n",
    "            event_start_idx = None\n",
    "            event_end = pattern_elements[i + 1]\n",
    "            event_end_idx = None\n",
    "            \n",
    "            # Check if these events already exist\n",
    "            for idx, event in enumerate(json_file['sankey']['nodes']):\n",
    "                if event['name'] == event_start:\n",
    "                    event_start_idx = idx\n",
    "                if event['name'] == event_end:\n",
    "                    event_end_idx = idx\n",
    "                    \n",
    "            # If start event not found\n",
    "            if event_start_idx == None:\n",
    "                json_file['sankey']['nodes'].append({\"name\": event_start})\n",
    "                event_start_idx = n_nodes\n",
    "                n_nodes += 1\n",
    "                \n",
    "            # If loop\n",
    "            if event_start == event_end:\n",
    "                event_end_idx = event_start_idx\n",
    "            # If end event not found\n",
    "            elif event_end_idx == None:\n",
    "                json_file['sankey']['nodes'].append({\"name\": event_end})\n",
    "                event_end_idx = n_nodes\n",
    "                n_nodes += 1\n",
    "\n",
    "            # Check if source->target already exists\n",
    "            for link in json_file['sankey']['links']:\n",
    "                if link['source'] == event_start_idx and link['target'] == event_end_idx:\n",
    "                    link['value'] += count\n",
    "                    break\n",
    "            else:\n",
    "                json_file['sankey']['links'].append({\n",
    "                    \"source\": event_start_idx,\n",
    "                    \"target\": event_end_idx,\n",
    "                    \"value\": count\n",
    "                })\n",
    "\n",
    "    return json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('sankey_data/dataset_pb_' + str(PROBLEM_ID) + '.json', 'w') as outfile:\n",
    "    json.dump(create_json_file(final_patterns), outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
